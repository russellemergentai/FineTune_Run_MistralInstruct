# -*- coding: utf-8 -*-
"""ONeillBot_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16cdIeQ43BZTO-cYMlkNrh3R3J8Qr6JIw

A T4 is fine.
ONeillBot is good at RAG, and commentary.
Not good at summarization or letter generation.
"""

!pip install langchain==0.1.17
!pip install -q -U bitsandbytes==0.43.1
!pip install accelerate==0.30.0
!pip install sentence-transformers==2.7.0
!pip install faiss-gpu==1.7.2

from langchain import hub
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from langchain.agents.agent_toolkits import create_retriever_tool
from langchain.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import DirectoryLoader

import torch #2.2.1
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model_id = "nachtwindecho/mistralai-Code-Instruct-Finetune-SG1-V4"
#model_id = "mistralai/Mistral-7B-Instruct-v0.1"

llm = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"":0})
tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)

from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

from google.colab import drive
drive.mount("/content/drive")

loader = TextLoader("/content/drive/MyDrive/Target/data_wf.txt", encoding = 'UTF-8')
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
chunk_size=200, chunk_overlap=50, separators=[" ", ",", "\n"]
)

#text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=25)
texts = text_splitter.split_documents(documents)

# add a retriever for RAG here
modelPath = "intfloat/e5-large-unsupervised"
embeddings = HuggingFaceEmbeddings(model_name = modelPath,  model_kwargs = {'device':'cuda'},encode_kwargs={'normalize_embeddings':False})

# Using faiss index
from langchain.vectorstores import FAISS
db = FAISS.from_documents(texts, embeddings)

retriever = db.as_retriever()

import re

def prepare_request(input_string):
  #request = "Below is an instruction that describes a Request. Write a Response that accurately completes the Request. \n\n Request: " + input_string

  prompt_template="""
  <s>
  [INST]
  Below is an instruction that describes a task. Write a response that appropriately completes the request.
  {query}
  [/INST]
  </s>
   <s>
  """
  request=prompt_template.format(query=input_string)

  return request

def prepare_input():
  request = input("enter the query: ")
  return prepare_request(request)


def contains_inst(sentence):
    return "[/INST]" in sentence#


def extract_answer(input_string):
    response = re.split('Helpful Answer:', input_string)[-1].strip()
    separator = "[/INST]"

    if contains_inst(sentence=separator):
      split_text = response.split(separator)
      formatted_text = ''.join(split_text)
      return formatted_text
    else:
      return response


def extract_single_line_answer(input_string):
    response = re.split('Helpful Answer:', input_string)[-1].strip()
    separator = "[/INST]"

    if contains_inst(sentence=separator):
      split_text = response.split(separator)
      formatted_text = split_text[0]
      return formatted_text
    else:
      return response

"""Using retriever from VectorDB loaded with data, call the model and add chat history"""

from langchain.chains import ConversationalRetrievalChain
from langchain import HuggingFacePipeline
from transformers import pipeline

ragPipeline = pipeline(
    "text-generation",
    model=llm,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map={"":0},
    max_length=500,
    #if True he model stochastically samples from the probability distribution over the vocabulary - good for RAG
    do_sample=True,
    top_k=5,
    num_return_sequences=1, # generate multiple alternative sequences or responses
    repetition_penalty=1.1,
    eos_token_id=tokenizer.eos_token_id
)

ragllm = HuggingFacePipeline(
    pipeline = ragPipeline,
    model_kwargs={"temperature": 0.2}
)


responsePipeline = pipeline(
    "text-generation",
    model=llm,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map={"":0},
    max_length=200,
    #if false he model stochastically samples from the probability distribution over the vocabulary - good for single response
    do_sample=True,
    num_return_sequences=1, # generate multiple alternative sequences or responses
    repetition_penalty=1.1,
    eos_token_id=tokenizer.eos_token_id
)

responsellm = HuggingFacePipeline(
    pipeline = responsePipeline,
    model_kwargs={"temperature": 0.2}
)

summaryPipeline = pipeline(
    "summarization",
    model=llm,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    device_map={"":0},
    max_length=750,
    eos_token_id=tokenizer.eos_token_id
)

summaryllm = HuggingFacePipeline(
    pipeline = summaryPipeline,
    model_kwargs={"temperature": 0.2}
)

"""RETRIEVAL QUERY:"""

chat_history = []
query = prepare_input()

qa = RetrievalQA.from_chain_type(llm=ragllm, retriever=retriever, return_source_documents=False)
result = qa.run({"query": query})

  #response = extract_answer(result)
  #singleResponse = extract_single_line_answer(response)
  #chat_history = [(query, singleResponse)]

print("RESULT>" + result)
  # print("RESPONSE>" + response)

"""REGULAR QUERY:"""

# Define the langchain pipeline for llm only
from langchain.prompts import PromptTemplate

#PROMPT_TEMPLATE ="""Below is an instruction that describes a Request. Write a Response that accurately answers the Request. \n\n Request: {question} \n\n
#Response:
#"""

PROMPT_TEMPLATE ="""
<s>
[INST]
Below is an instruction that describes a task. Write a response that appropriately completes the request.
{question}
[/INST]
</s>
 <s>
  """

PROMPT = PromptTemplate.from_template(PROMPT_TEMPLATE)
llm_chain = PROMPT | responsellm
query = input("enter the query: ")
result = llm_chain.invoke({"question": query})

print(result.rstrip())

"""SUMMARIZE QUERY:"""

# Define the langchain pipeline for llm only
from langchain.prompts import PromptTemplate

PROMPT_TEMPLATE ="""Below is an instruction that describes a Request. Write a Response that accurately answers the Request. Review your answer to ensure it is accurate\n\n Request: {question} \n\n
Response:
"""

PROMPT = PromptTemplate.from_template(PROMPT_TEMPLATE)
llm_chain = PROMPT | summaryllm
#query = input("enter the query: ")

query = "summarize the following text: \
Sparta, also known as Lacedaemon, was an ancient Greek city-state located primarily in a region of southern Greece called Laconia. The population of Sparta consisted of three main groups: the Spartans, or Spartiates, who were full citizens; the Helots, or serfs/slaves; and the Perioeci, who were neither slaves nor citizens. The Perioeci, whose name means “dwellers-around,” worked as craftsmen and traders, and made weapons for the Spartans. \
Did you know? The word “spartan” means self-restrained, simple, frugal and austere. The word laconic, which means pithy and concise, is derived from the Spartans, who prized brevity of speech. \
All healthy male Spartan citizens participated in the compulsory state-sponsored education system, the Agoge, which emphasized obedience, endurance, courage and self-control. Spartan men devoted their lives to military service, and lived communally well into adulthood. A Spartan was taught that loyalty to the state came before everything else, including one’s family. \
The Helots, whose name means “captives,” were fellow Greeks, originally from Laconia and Messenia, who had been conquered by the Spartans and turned into slaves. The Spartans’ way of life would not have been possible without the Helots, who handled all the day-to-day tasks and unskilled labor required to keep society functioning: They were farmers, domestic servants, nurses and military attendants. \
Spartans, who were outnumbered by the Helots, often treated them brutally and oppressively in an effort to prevent uprisings. Spartans would humiliate the Helots by doing such things as forcing them to get debilitatingly drunk on wine and then make fools of themselves in public. (This practice was also intended to demonstrate to young people how an adult Spartan should never act, as self-control was a prized trait.) \
Methods of mistreatment could be far more extreme: Spartans were allowed to kill Helots for being too smart or too fit, among other reasons."

result = llm_chain.invoke({"question": query})

print(result.rstrip())